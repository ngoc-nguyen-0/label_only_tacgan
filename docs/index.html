<!doctype html>
<title>Re-thinking Model Inversion Attacks Against Deep Neural Networks</title>
    <html lang="en">

<head>
    <meta charset="UTF-8">
    <!-- If IE use the latest rendering engine -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- Set the page to the width of the device and set the zoon level -->
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="style.css">

</head>


<body>
    <!-- page-header-->
    <div class="jumbotron" style="background-color:#fefefe;">
        <div class="container" style="font-family:'Times New Roman',serif;margin-bottom:-35px;margin-top:-20px">
            <h1 class="display-1" style="text-align:center; font-weight:bold; font-size:2.0em;letter-spacing:2.0px;">
                Label-Only Model Inversion Attacks via Knowledge Transfer</h1>
            <hr style="border-top: 1px solid #000000; background: transparent;" class="my-4">
            
            <p class="lead" style="text-align:center;font-size:1.25em;">
                <a href="https://scholar.google.com/citations?user=zQPES6kAAAAJ&hl=vi" font color="#ffffff">Ngoc-Bao Nguyen<sup>*1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://keshik6.github.io/">Keshigeyan Chandrasegaran<sup>*2&Dagger;</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://miladabd.github.io/">Milad Abdollahzaden<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://sites.google.com/site/mancheung0407/">Ngai-Man Cheung<sup>1</sup></a> </br>
                <sup>1 Singapore University of Technology and Design (SUTD) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <sup>2 Stanford University.  <br/>
            
            <b><em>Neurips 2023</br></em></b>
            </p>
                
                
            <p class="lead" style="text-align:center;font-size:1.25em;"> 
                <a href="https://github.com/ngoc-nguyen-0/LOKT_neurips2023">Paper</a> |
                <a href="https://github.com/ngoc-nguyen-0/LOKT_neurips2023">Github</a> |
                <a href="https://github.com/ngoc-nguyen-0/LOKT_neurips2023">Models</a> |            
                <a href="https://github.com/ngoc-nguyen-0/LOKT_neurips2023">Demo</a> 
            </p>         
                
        </div>
    </div>
    
    <!-- Abstract -->
    <div class="container" style="font-family:'Times New Roman',serif">
        <h2 class="display-1" style="text-align:center; font-weight:bold; font-size:1.5em;letter-spacing:2.0px;">Abstract</h2>
        <hr style="border-top: 1px solid #000000; background: transparent;" class="my-4">
            <p class="lead" style="text-align:left;font-size:1.25em;text-align:justify;text-justify:inter-word;">  
                In a model inversion (MI) attack, an adversary abuses access to a machine learning (ML) model to infer and reconstruct private training data. Remarkable progress has been made in the white-box and black-box setups, where the adversary has access to the complete model or the model's soft output respectively. However, there is very limited study in the most challenging but practically important setup: Label-only MI attacks, where the adversary only has access to the model's predicted label (hard label) without confidence scores nor any other model information.
            </p>
            <br>
            <p class="lead" style="text-align:left;font-size:1.25em;text-align:justify;text-justify:inter-word;">
                In this work, we propose LOKT, a novel approach for label-only MI attacks. Our idea is based on transfer of knowledge from the opaque target model to surrogate models. Subsequently, using these surrogate models, our approach can harness advanced white-box attacks. We propose knowledge transfer based on generative modelling, and introduce a new model, Target model-assisted ACGAN (T-ACGAN), for effective knowledge transfer. Our method casts the challenging label-only MI into the more tractable white-box setup. We provide analysis to support that surrogate models based on our approach serve as effective proxies for the target model for MI. Our experiments show that our method significantly outperforms existing SOTA Label-only MI attack by more than 15% across all MI benchmarks. Furthermore, our method compares favorably in terms of query budget. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed.    
            </p>

    </div>

    </br>

    <!-- Proposed Approach -->
    <div class="container" style="font-family:'Times New Roman',serif" >
        <figure>                
            <div class="col-md-12 text-center">
                <img src="./assets/images/framework.jpg" alt="framework" style="width:80%;height:80%;">
            </div>

            <figcaption style="text-align:justify;text-justify:inter-word;"> Figure1: Overview and our contributions.
                (a) Under Label-only model inversion (MI) attack, the Target model T is opaque.
                (b) Stage 1: As our first contribution, we propose a knowledge transfer scheme to render surrogate model(s). 
                (b) Stage 2: Then, we cast the Label-only MI attack as a white-box MI attack on surrogate model(s) S.
                (c) This casting can ease the challenging problem setup of label-only MI attack into a white-box MI attack. To our knowledge, our proposed approach is the first to address label-only MI via white-box MI attacks.
                (d) We propose T-ACGAN to leverage generative modeling and the target model for effective knowledge transfer to render surrogate model(s). Knowledge transfer renders D (Discriminator) as a surrogate model, and further generated samples of T-ACGAN can be used to train other  surrogate variant S.
                (e) Our analysis demonstrates that S is a good proxy for T for MI attack.
                In particular, white-box MI attack on S mimics the white-box attack on opaque T. 
                (f) Our proposed approach significantly improves the Label-only MI attack (e.g. 20% improvement in standard CelebA benchmark compared to existing SOTA resulting in significant improvement in private data reconstruction.
            </figcaption>
        </figure>
    </div>


    <!-- Citation -->
    <div class="container" style="font-family:'Times New Roman',serif">
        <h2 class="display-1" style="text-align:center; font-weight:bold; font-size:1.5em;letter-spacing:2.0px;">Citation</h2>
        <hr style="border-top: 1px solid #000000; background: transparent;" class="my-4">
            
        <pre class="lead" style="text-align:left;font-size:1.0em;white-space: pre-line;text-align:justify;text-justify:inter-word;">
            <code>@InProceedings{nguyen2023labelonly,
    author    = {Nguyen, Ngoc-Bao and Chandrasegaran, Keshigeyan and Abdollahzadeh, Milad and Cheung, Ngai-Man},
    title     = {Label-Only Model Inversion Attacks via Knowledge Transfer},
    booktitle = {Advances in Neural Information Processing Systems},
    month     = {Dec},
    year      = {2023}
}</code>
        </pre>
    </div>


    <!-- Acknowledgements -->
    <div class="container" style="font-family:'Times New Roman',serif">
        <h2 class="display-1" style="text-align:center; font-weight:bold; font-size:1.5em;letter-spacing:2.0px;">Acknowledgements</h2>
        <hr style="border-top: 1px solid #000000; background: transparent;" class="my-4">
            
            <p class="lead" style="text-align:left;font-size:1.25em;text-align:justify;text-justify:inter-word;">
                This research is supported by the National Research Foundation, Singapore under its AI Singapore Programmes
(AISG Award No.: AISG2-TC-2022-007) and SUTD project PIE-SGP-AI-2018-01. 
This research work is also supported by the Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021). 
This material is based on the research/work support in part by the Changi General Hospital and Singapore University of Technology and Design, under the HealthTech Innovation Fund (HTIF Award No. CGH-SUTD-2021-004).
We thank anonymous reviewers for their insightful feedback and discussion.
</p>
    </div>
    <div class="container" style="font-family:'Times New Roman',serif">
        <hr style="border-top: 1px solid #000000; background: transparent;" class="my-4">            
            <p class="lead" style="text-align:left;font-size:1.25em;">
            <em> <sup>* These authors contributed equally  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <sup>&Dagger; Work done while at SUTD.</em> <br/>

    </p>
    </div>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

</body>
</html>
